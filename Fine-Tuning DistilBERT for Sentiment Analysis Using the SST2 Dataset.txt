# -*- coding: utf-8 -*-
"""Chapter5_Solution1_DistilBERT_SST2_Transfer_Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13QNE3i0XsEp4MffL45NTwX_A9cPAjPCj

# Sentiment Analysis with DistilBERT and SST2 Dataset
This notebook demonstrates how to perform sentiment analysis using a pre-trained DistilBERT model fine-tuned on the SST2 dataset from Hugging Face.
"""

# Install necessary packages
!pip install transformers datasets tensorflow

"""## Import Libraries
We begin by importing the necessary libraries.
"""

import tensorflow as tf
from transformers import TFAutoModelForSequenceClassification, AutoTokenizer
from datasets import load_dataset

"""## Load the SST2 Dataset
Next, we load the SST2 dataset from Hugging Face.
"""

# Load the SST2 dataset
dataset = load_dataset("stanfordnlp/sst2")

"""## Load the DistilBERT Tokenizer
We load the pre-trained DistilBERT tokenizer to process the input text.
"""

# Load the DistilBERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

"""## Tokenize the Dataset
We define a function to tokenize the dataset and apply it to the SST2 dataset.
"""

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples["sentence"], padding='max_length', truncation=True, max_length=128, return_tensors='tf')

tokenized_datasets = dataset.map(tokenize_function, batched=True)

"""## Convert to TensorFlow Dataset
We convert the tokenized dataset to a format that can be used with TensorFlow.
"""

# Convert the tokenized dataset to a TensorFlow dataset
train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask"],
    label_cols="label",
    shuffle=True,
    batch_size=64
)

validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["input_ids", "attention_mask"],
    label_cols="label",
    shuffle=False,
    batch_size=64
)

for batch in train_dataset.take(1):
    print(batch[0]['input_ids'].shape)
    print(batch[0]['attention_mask'].shape)
    print(batch[1].shape)
    break

"""## Load and Configure the DistilBERT Model
We load the pre-trained DistilBERT model and configure it for sequence classification.
"""

# Load the pre-trained DistilBERT model
model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)

"""## Freeze DistilBERT Layers
We freeze the DistilBERT layers to focus training on the classification layer.
"""

# Freeze the DistilBERT layers
model.layers[0].trainable = False

model.summary()

"""## Compile the Model
We compile the model with appropriate loss function, optimizer, and metrics.
"""

# Compile the model
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=tf.metrics.SparseCategoricalAccuracy(),
)

"""## Train the Model
We train the model using the training dataset and validate it using the validation dataset.
"""

# Train the model
model.fit(
    train_dataset,
    validation_data=validation_dataset,
    epochs=3
)

"""## Save the Model
We save the trained model for future use.
"""

# Save the model
model.save_pretrained("./distilbert-sst2")

"""## Evaluate the Model
Finally, we evaluate the model using the test dataset to check its performance.
"""

results = model.evaluate(validation_dataset)
print(f"Test loss: {results[0]}")
print(f"Test accuracy: {results[1]}")

